{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning `BERT` for `Disaster Tweets` Classification\n",
    "\n",
    "\n",
    "Text classification is a technique for putting text into different categories and has a wide range of applications: email providers use text classification to detect to spam emails, marketing agencies use it for sentiment analysis of customer reviews, and moderators of discussion forums use it to detect inappropriate comments.\n",
    "\n",
    "In the past, data scientists used methods such as [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [word2vec](https://en.wikipedia.org/wiki/Word2vec), or [bag-of-words (BOW)](https://en.wikipedia.org/wiki/Bag-of-words_model) to generate features for training classification models. While these techniques have been very successful in many NLP tasks, they don't always capture the meanings of words accurately when they appear in different contexts. Recently, we see increasing interest in using [Bidirectional Encoder Representations from Transformers (BERT)](https://arxiv.org/abs/1810.04805) to achieve better results in text classification tasks, due to its ability more accurately encode the meaning of words in different contexts.\n",
    "\n",
    "BERT was trained on BookCorpus and English Wikipedia data, which contain 800 million words and 2,500 million words, respectively. Training BERT from scratch would be prohibitively expensive. By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a relatively small amount of training data to achieve state-of-the-art results for common NLP tasks, such as text classification and question answering. \n",
    "\n",
    "[Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/index.html) is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) provides open source APIs and containers that make it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks.\n",
    "\n",
    "In this example, we walk through our dataset, the training process, and finally model deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the `Problem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies). However, identifying such tweets has always been a difficult task because of the ambiguity in the linguistic structure of the tweets and hence it is not always clear whether an individual’s words are actually announcing a disaster.\n",
    "\n",
    "More details [here](https://www.kaggle.com/c/nlp-getting-started/overview)\n",
    "\n",
    "<img src = \"img/disaster.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To start, we import some Python libraries and initialize a SageMaker session, S3 bucket and prefix, and IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()    # Provides a collection of methods for working with SageMaker resources\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-bert\"\n",
    "\n",
    "role = sagemaker.get_execution_role()      # Get the execution role for the notebook instance. \n",
    "                                           # This is the IAM role that we created for our notebook instance. \n",
    "                                           # We pass the role to the tuning job(later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "\n",
    "Kaggle hosted a challenge named `Real` or `Not` whose aim was to use the Twitter data of disaster tweets, originally created by the company figure-eight, to classify Tweets talking about `real disaster` against the ones talking about it metaphorically. (https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `sentences` and `labels`\n",
    "\n",
    "Let us take a quick look at our data and for that we need to first read the training data. \n",
    "The only two columns we interested are:\n",
    "\n",
    "- the `sentence` (the tweet)\n",
    "- the `label`    (the label, this denotes whether a tweet is about a real disaster (1) or not (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"dataset/raw/data.csv\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "\n",
    "\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>1</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>1</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>1</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>1</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>1</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence\n",
       "7608      1  Two giant cranes holding a bridge collapse int...\n",
       "7609      1  @aria_ahrary @TheTawniest The out of control w...\n",
       "7610      1  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611      1  Police investigating after an e-bike collided ...\n",
       "7612      1  The Latest: More Homes Razed by Northern Calif..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing few tweets with its class label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"mom: 'we didn't get home as fast as we wished' \\nme: 'why is that?'\\nmom: 'there was an accident and some truck spilt mayonnaise all over ??????\",\n",
       "  0),\n",
       " (\"I was in a horrible car accident this past Sunday. I'm finally able to get around. Thank you GOD??\",\n",
       "  1),\n",
       " ('Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident??',\n",
       "  0),\n",
       " (\"#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k\",\n",
       "  1),\n",
       " ('Accident in #Ashville on US 23 SB before SR 752 #traffic http://t.co/hylMo0WgFI',\n",
       "  1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sentences[80:85], labels[80:85]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above output, there are few information which are not that important, like `URLs`, `Emojis`, `Tags`, etc. So, now lets try to clean the dataset before we actually pass this data for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to clean text by removing urls, emojis, html tags and punctuations.\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentence'].apply(lambda x: remove_URL(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_emoji(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_html(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Our Deeds are the Reason of this earthquake Ma...\n",
       "1      1              Forest fire near La Ronge Sask Canada\n",
       "2      1  All residents asked to shelter in place are be...\n",
       "3      1  13000 people receive wildfires evacuation orde...\n",
       "4      1  Just got sent this photo from Ruby Alaska as s..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mom we didnt get home as fast as we wished \\nme why is that\\nmom there was an accident and some truck spilt mayonnaise all over ',\n",
       "  0),\n",
       " ('I was in a horrible car accident this past Sunday Im finally able to get around Thank you GOD',\n",
       "  1),\n",
       " ('Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident',\n",
       "  0),\n",
       " ('TruckCrash Overturns On FortWorth Interstate  Click here if youve been in a crash',\n",
       "  1),\n",
       " ('Accident in Ashville on US 23 SB before SR 752 traffic ', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sentences[80:85], labels[80:85]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Let's spend couple of minutes to explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Data Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\n",
    "sns.countplot(df['label'], ax=axes[0])\n",
    "axes[1].pie(df['label'].value_counts(),\n",
    "            labels=['Non Disaster', 'Real Disaster'],\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=True,\n",
    "            explode=(0.05, 0),\n",
    "            startangle=60)\n",
    "fig.suptitle('Distribution of the Tweets', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset for `training` and `testing`\n",
    "\n",
    "We then split the dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)               # Default split ratio 75/25, we can modify using \"test_size\"\n",
    "train.to_csv(\"dataset/train.csv\", index=False)\n",
    "test.to_csv(\"dataset/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload both to Amazon S3 for use later\n",
    "\n",
    "The SageMaker Python SDK provides a helpful function for uploading to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"dataset/train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"dataset/test.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker\n",
    "\n",
    "- When running a training job, SageMaker reads input data from Amazon S3, uses that data to train a model. \n",
    "- Training data from S3 is made available to the Model Training instance container, which is pulled from Amazon Elastic Container Registry(`ECR`). \n",
    "- The training job persists model artifacts back to the output S3 location designated in the training job configuration. \n",
    "- When we are ready to deploy a model, SageMaker spins up new ML instances and pulls in these model artifacts to use for batch or real-time model inference.\n",
    "\n",
    "<img src = \"img/sm-training.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script\n",
    "\n",
    "We use the [PyTorch-Transformers library](https://pytorch.org/hub/huggingface_pytorch-transformers), which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called `model_dir`, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in `model_dir` will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named `train_deploy.py`, and put the file in a directory named `code/`. The full training script can be viewed under `code/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader, RandomSampler, TensorDataset\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AdamW, BertForSequenceClassification, BertTokenizer\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mast\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mitr\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "MAX_LEN = \u001b[34m64\u001b[39;49;00m  \u001b[37m# this is the max length of the sentence\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading BERT tokenizer...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, do_lower_case=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mflat_accuracy\u001b[39;49;00m(preds, labels):\r\n",
      "    pred_flat = np.argmax(preds, axis=\u001b[34m1\u001b[39;49;00m).flatten()\r\n",
      "    labels_flat = labels.flatten()\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m np.sum(pred_flat == labels_flat) / \u001b[36mlen\u001b[39;49;00m(labels_flat)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    sentences = dataset.sentence.values\r\n",
      "    labels = dataset.label.values\r\n",
      "\r\n",
      "    input_ids = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\r\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        input_ids.append(encoded_sent)\r\n",
      "\r\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\r\n",
      "    input_ids_padded = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\r\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        input_ids_padded.append(i)\r\n",
      "    input_ids = input_ids_padded\r\n",
      "\r\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\r\n",
      "    attention_masks = []\r\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\r\n",
      "        attention_masks.append(att_mask)\r\n",
      "\r\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\r\n",
      "    train_inputs = torch.tensor(input_ids)\r\n",
      "    train_labels = torch.tensor(labels)\r\n",
      "    train_masks = torch.tensor(attention_masks)\r\n",
      "\r\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        train_sampler = RandomSampler(train_data)\r\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir):\r\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    sentences = dataset.sentence.values\r\n",
      "    labels = dataset.label.values\r\n",
      "\r\n",
      "    input_ids = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\r\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        input_ids.append(encoded_sent)\r\n",
      "\r\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\r\n",
      "    input_ids_padded = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\r\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        input_ids_padded.append(i)\r\n",
      "    input_ids = input_ids_padded\r\n",
      "\r\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\r\n",
      "    attention_masks = []\r\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\r\n",
      "        attention_masks.append(att_mask)\r\n",
      "\r\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\r\n",
      "    train_inputs = torch.tensor(input_ids)\r\n",
      "    train_labels = torch.tensor(labels)\r\n",
      "    train_masks = torch.tensor(attention_masks)\r\n",
      "\r\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
      "    train_sampler = RandomSampler(train_data)\r\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    \u001b[37m# PyTorch DDP cluster configuration\u001b[39;49;00m\r\n",
      "    \u001b[37m# ------------------------------------------------------------    \u001b[39;49;00m\r\n",
      "    \u001b[37m# rank ID within a node (unique per machine, not per cluster)\u001b[39;49;00m\r\n",
      "    local_rank = args.local_rank\r\n",
      "    \r\n",
      "    \u001b[37m# determine global rank - rank ID within the cluster (unique per machine and per cluster)\u001b[39;49;00m\r\n",
      "    rank_list = [\u001b[36mstr\u001b[39;49;00m(i) + \u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(j) \r\n",
      "                 \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itr.product(\u001b[36mrange\u001b[39;49;00m(\u001b[34m0\u001b[39;49;00m, args.nodes), \u001b[36mrange\u001b[39;49;00m(\u001b[34m0\u001b[39;49;00m, args.gpus))]\r\n",
      "    \r\n",
      "    global_rank_str = \u001b[36mstr\u001b[39;49;00m(args.node_rank) + \u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(local_rank)\r\n",
      "    \r\n",
      "    global_rank = rank_list.index(global_rank_str)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU local rank \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlocal_rank\u001b[33m}\u001b[39;49;00m\u001b[33m on machine \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.node_rank\u001b[33m}\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\r\n",
      "          \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mis given global rank \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mglobal_rank\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMASTER_ADDR \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mMASTER_ADDR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMASTER_PORT \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mMASTER_PORT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    dist.init_process_group(args.backend, rank=global_rank, world_size=args.nodes*args.gpus)\r\n",
      "    \r\n",
      "    \r\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\r\n",
      "  \r\n",
      "    torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Instantiate model\u001b[39;49;00m\r\n",
      "    \u001b[37m# ------------------------------------------------------------\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m# One replica downloads the model, the other replicas read it from hub cache\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m global_rank == \u001b[34m0\u001b[39;49;00m:\r\n",
      "        model = BertForSequenceClassification.from_pretrained(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m# Use the 12-layer BERT model, with an uncased vocab.\u001b[39;49;00m\r\n",
      "        num_labels=args.num_labels,  \u001b[37m# The number of output labels--2 for binary classification.\u001b[39;49;00m\r\n",
      "        output_attentions=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns attentions weights.\u001b[39;49;00m\r\n",
      "        output_hidden_states=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns all hidden-states.\u001b[39;49;00m\r\n",
      "        )\r\n",
      "    dist.barrier()\r\n",
      "        \r\n",
      "    \u001b[34mif\u001b[39;49;00m global_rank != \u001b[34m0\u001b[39;49;00m:\r\n",
      "        model = BertForSequenceClassification.from_pretrained(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m# Use the 12-layer BERT model, with an uncased vocab.\u001b[39;49;00m\r\n",
      "        num_labels=args.num_labels,  \u001b[37m# The number of output labels--2 for binary classification.\u001b[39;49;00m\r\n",
      "        output_attentions=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns attentions weights.\u001b[39;49;00m\r\n",
      "        output_hidden_states=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns all hidden-states.\u001b[39;49;00m\r\n",
      "        )\r\n",
      "  \r\n",
      "    device = torch.device(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlocal_rank\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = model.to(device)\r\n",
      "\r\n",
      "        \r\n",
      "    model = torch.nn.parallel.DistributedDataParallel(model,device_ids=[local_rank],broadcast_buffers=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "   \r\n",
      "    optimizer = AdamW(\r\n",
      "        model.parameters(),\r\n",
      "        lr=\u001b[34m2e-5\u001b[39;49;00m,  \u001b[37m# args.learning_rate - default is 5e-5, our notebook had 2e-5\u001b[39;49;00m\r\n",
      "        eps=\u001b[34m1e-8\u001b[39;49;00m,  \u001b[37m# args.adam_epsilon - default is 1e-8.\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnd of defining BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "        model.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m step, batch \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\r\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\r\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\r\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\r\n",
      "            model.zero_grad()\r\n",
      "\r\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask, labels=b_labels)\r\n",
      "            loss = outputs[\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "            total_loss += loss.item()\r\n",
      "            loss.backward()\r\n",
      "            torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "            \u001b[37m# modified based on their gradients, the learning rate, etc.\u001b[39;49;00m\r\n",
      "            optimizer.step()\r\n",
      "            \u001b[34mif\u001b[39;49;00m step % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        step * \u001b[36mlen\u001b[39;49;00m(batch[\u001b[34m0\u001b[39;49;00m]),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * step / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mAverage training loss: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader))\r\n",
      "\r\n",
      "        test(model, test_loader, device)\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving tuned model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model_2_save = model.module \u001b[34mif\u001b[39;49;00m \u001b[36mhasattr\u001b[39;49;00m(model, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodule\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34melse\u001b[39;49;00m model\r\n",
      "    model_2_save.save_pretrained(save_directory=args.model_dir)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\r\n",
      "    model.eval()\r\n",
      "    _, eval_accuracy = \u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\r\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\r\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\r\n",
      "\r\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask)\r\n",
      "            logits = outputs[\u001b[34m0\u001b[39;49;00m]\r\n",
      "            logits = logits.detach().cpu().numpy()\r\n",
      "            label_ids = b_labels.to(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).numpy()\r\n",
      "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
      "            eval_accuracy += tmp_eval_accuracy\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Accuracy: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, tmp_eval_accuracy)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ objects in model_dir ===================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(model_dir))\r\n",
      "    model = BertForSequenceClassification.from_pretrained(model_dir)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ model loaded ===========================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\r\n",
      "    \u001b[33m\"\"\"An input_fn that loads a pickled tensor\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        data = json.loads(request_body)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ input sentences ===============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(data)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mstr\u001b[39;49;00m):\r\n",
      "            data = [data]\r\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mlist\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(data) > \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data[\u001b[34m0\u001b[39;49;00m], \u001b[36mstr\u001b[39;49;00m):\r\n",
      "            \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported input type. Input type can be a string or an non-empty list. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m                             I got \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data))\r\n",
      "                       \r\n",
      "        \u001b[37m#encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\u001b[39;49;00m\r\n",
      "        \u001b[37m#encoded = tokenizer(data, add_special_tokens=True) \u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# for backward compatibility use the following way to encode \u001b[39;49;00m\r\n",
      "        \u001b[37m# https://github.com/huggingface/transformers/issues/5580\u001b[39;49;00m\r\n",
      "        input_ids = [tokenizer.encode(x, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m data]\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ encoded sentences ==============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(input_ids)\r\n",
      "\r\n",
      "        \u001b[37m# pad shorter sentence\u001b[39;49;00m\r\n",
      "        padded =  torch.zeros(\u001b[36mlen\u001b[39;49;00m(input_ids), MAX_LEN) \r\n",
      "        \u001b[34mfor\u001b[39;49;00m i, p \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(input_ids):\r\n",
      "            padded[i, :\u001b[36mlen\u001b[39;49;00m(p)] = torch.tensor(p)\r\n",
      "     \r\n",
      "        \u001b[37m# create mask\u001b[39;49;00m\r\n",
      "        mask = (padded != \u001b[34m0\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================= padded input and attention mask ================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(padded, \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mask)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m padded.long(), mask.long()\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported content type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(request_content_type))\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model.to(device)\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    input_id, input_mask = input_data\r\n",
      "    input_id = input_id.to(device)\r\n",
      "    input_mask = input_mask.to(device)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m============== encoded data =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(input_id, input_mask)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        y = model(input_id, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=============== inference result =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(y)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m y\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Adapt arg defaults if this script runs on SageMaker\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m os.environ:\r\n",
      "        hosts = ast.literal_eval(os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "        default_nodes = \u001b[36mlen\u001b[39;49;00m(hosts)\r\n",
      "        default_node_rank = hosts.index(os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        default_nodes = \u001b[34m1\u001b[39;49;00m\r\n",
      "        default_node_rank = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \r\n",
      "  \r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    \r\n",
      "    \r\n",
      "     \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--num_labels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m50\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \r\n",
      "\r\n",
      "    \u001b[37m# infra configuration\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m6\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--prefetch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--amp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--nodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=default_nodes)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--node_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=default_node_rank)    \r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--local_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)  \u001b[37m# set by PyTorch DDP launch utility\u001b[39;49;00m\r\n",
      "\r\n",
      "  \r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \r\n",
      "    torch.cuda.empty_cache()\r\n",
      "    \r\n",
      "    \r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Amazon SageMaker\n",
    "\n",
    "We use Amazon SageMaker to train and deploy a model using our custom PyTorch code. The Amazon SageMaker Python SDK makes it easier to run a PyTorch script in Amazon SageMaker using its PyTorch estimator. After that, we can use the SageMaker Python SDK to deploy the trained model and run predictions. For more information on how to use this SDK with PyTorch, see [the SageMaker Python SDK documentation](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html).\n",
    "\n",
    "To start, we use the `PyTorch` estimator class to train our model. When creating our estimator, we make sure to specify a few things:\n",
    "\n",
    "* `entry_point`: the name of our PyTorch script. It contains our training script, which loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. It also contains code to load and run the model during inference.\n",
    "* `source_dir`: the location of our training scripts and requirements.txt file. \"requirements.txt\" lists packages you want to use with your script.\n",
    "* `framework_version`: the PyTorch version we want to use\n",
    "\n",
    "The PyTorch estimator supports multi-machine, distributed PyTorch training. To use this, we just set train_instance_count to be greater than one. Our training script supports distributed training for only GPU instances. \n",
    "\n",
    "After creating the estimator, we then call fit(), which launches a training job. We use the Amazon S3 URIs where we uploaded the training data earlier.\n",
    "\n",
    "<img src = \"img/sm-estimator.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2.106.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.107.0.tar.gz (568 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.4/568.4 KB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<22,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.24.42)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.42 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.48)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.42->boto3<2.0,>=1.20.21->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.107.0-py2.py3-none-any.whl size=784128 sha256=ac3f93c6b42671572f7379a07c06a824562336ec9e327a9d8becbdd141f76f31\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/49/84/d6/be7707e36d08706ead27ebc97a732eff8b739f05b1387bf8aa\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.106.0\n",
      "    Uninstalling sagemaker-2.106.0:\n",
      "      Successfully uninstalled sagemaker-2.106.0\n",
      "Successfully installed sagemaker-2.107.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-31 23:03:44 Starting - Starting the training job.........\n",
      "2022-08-31 23:04:51 Starting - Preparing the instances for training.........\n",
      "2022-08-31 23:06:39 Downloading - Downloading input data...\n",
      "2022-08-31 23:06:58 Training - Downloading the training image...........................\n",
      "2022-08-31 23:11:38 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:44,200 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:44,650 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:44,727 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:44,730 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:44,730 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:44,282 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:44,285 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:44,285 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:44,809 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:45,349 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\u001b[0m\n",
      "\u001b[34mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 6.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.64.0)\u001b[0m\n",
      "\u001b[35mCollecting requests==2.22.0\u001b[0m\n",
      "\u001b[35mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 6.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.8.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 768.9/768.9 kB 35.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 51.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 40.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting regex\u001b[0m\n",
      "\u001b[35mDownloading regex-2022.8.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 768.9/768.9 kB 43.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 54.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sacremoses\u001b[0m\n",
      "\u001b[35mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 50.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.4.3)\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 70.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 11.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting idna<2.9,>=2.5\u001b[0m\n",
      "\u001b[34mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 kB 12.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting chardet<3.1.0,>=3.0.2\u001b[0m\n",
      "\u001b[34mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (1.22.2)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.4.3)\u001b[0m\n",
      "\u001b[35mCollecting transformers\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 70.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\u001b[0m\n",
      "\u001b[35mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 9.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting chardet<3.1.0,>=3.0.2\u001b[0m\n",
      "\u001b[35mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 kB 14.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting idna<2.9,>=2.5\u001b[0m\n",
      "\u001b[35mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 kB 11.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2022.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (1.22.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 73.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.8.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.7/120.7 kB 11.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 7)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 7)) (3.0.9)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.7/120.7 kB 7.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 62.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35mDownloading filelock-3.8.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 7)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 7)) (3.0.9)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[35mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=6512d2f338c9dfc59321c04782f1a5e559ab08c7399f9e3b3f0b4fe112679ba0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[35mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=a0e1c7f6cafe9de30778297f7c5519cd1d51e1a1537c3c2967f90d8242cc6a24\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[35mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, sentencepiece, chardet, urllib3, regex, idna, filelock, sacremoses, requests, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, sentencepiece, chardet, urllib3, regex, idna, filelock, sacremoses, requests, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.11\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.11:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.11\u001b[0m\n",
      "\u001b[34mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[34mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[34mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.28.1\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.28.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[35mFound existing installation: urllib3 1.26.11\u001b[0m\n",
      "\u001b[35mUninstalling urllib3-1.26.11:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled urllib3-1.26.11\u001b[0m\n",
      "\u001b[35mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[35mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[35mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[35mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[35mFound existing installation: requests 2.28.1\u001b[0m\n",
      "\u001b[35mUninstalling requests-2.28.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled requests-2.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed chardet-3.0.4 filelock-3.8.0 huggingface-hub-0.9.1 idna-2.8 regex-2022.8.17 requests-2.22.0 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.2 urllib3-1.25.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,797 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,797 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,952 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,953 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,959 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,961 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:54,961 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35mSuccessfully installed chardet-3.0.4 filelock-3.8.0 huggingface-hub-0.9.1 idna-2.8 regex-2022.8.17 requests-2.22.0 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.2 urllib3-1.25.11\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,420 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,420 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,578 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,578 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,590 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,683 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,684 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,684 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,684 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:55,691 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:55,974 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,065 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,065 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,066 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,066 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,066 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,066 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,067 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,067 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2022-08-31 23:11:56,147 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_pytorch_ddp_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 2,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-08-31-23-03-44-023\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-365792799466/pytorch-training-2022-08-31-23-03-44-023/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"nccl\",\"epochs\":2,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_pytorch_ddp_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-365792799466/pytorch-training-2022-08-31-23-03-44-023/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":2,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-08-31-23-03-44-023\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-365792799466/pytorch-training-2022-08-31-23-03-44-023/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"2\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py train_deploy.py --backend nccl --epochs 2 --num_labels 2\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:56,694 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=142, name='orted', status='running', started='23:11:56')]\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:56,694 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=142, name='orted', status='disk-sleep', started='23:11:56')]\u001b[0m\n",
      "\u001b[35m2022-08-31 23:11:56,695 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=142, name='orted', status='disk-sleep', started='23:11:56')]\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.94.10' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Loading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 52.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 56.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 41.1kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 24.6kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 713kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 886kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:local rank is  0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:global rank is ******** 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:local rank is  5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:global rank is ******** 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:local rank is  6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:global rank is ******** 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:local rank is  1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:global rank is ******** 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:local rank is  7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:global rank is ******** 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:local rank is  4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:global rank is ******** 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:local rank is  6\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:global rank is ******** 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:local rank is  7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:global rank is ******** 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:local rank is  3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:global rank is ******** 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:local rank is  2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:global rank is ******** 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:local rank is  2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:global rank is ******** 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:local rank is  4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:global rank is ******** 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:local rank is  5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:global rank is ********[1,mpirank:13,algo-2]<stdout>: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:local rank is  1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:global rank is ******** 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:local rank is  0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:global rank is ******** 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:local rank is  3\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:world size is  16\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:global rank is ******** 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Processes 357/5709 (6%) of train data\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Processes 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 4.89M/420M [00:00<00:08, 51.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 4.37M/420M [00:00<00:09, 45.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 10.1M/420M [00:00<00:08, 53.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 10.6M/420M [00:00<00:07, 57.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   4%|▎         | 15.4M/420M [00:00<00:07, 54.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 16.9M/420M [00:00<00:06, 61.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   5%|▍         | 20.8M/420M [00:00<00:07, 55.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 22.9M/420M [00:00<00:06, 61.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   6%|▋         | 26.3M/420M [00:00<00:07, 56.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 28.8M/420M [00:00<00:06, 62.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 31.8M/420M [00:00<00:07, 56.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 35.6M/420M [00:00<00:06, 65.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 37.2M/420M [00:00<00:07, 56.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 42.4M/420M [00:00<00:05, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 42.6M/420M [00:00<00:07, 55.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 49.2M/420M [00:00<00:05, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  11%|█▏        | 47.9M/420M [00:00<00:06, 55.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 55.9M/420M [00:00<00:05, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 62.7M/420M [00:01<00:05, 69.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 53.3M/420M [00:01<00:07, 54.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 69.5M/420M [00:01<00:05, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 58.4M/420M [00:01<00:07, 51.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 76.3M/420M [00:01<00:05, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 63.4M/420M [00:01<00:07, 50.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 83.1M/420M [00:01<00:04, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 68.2M/420M [00:01<00:07, 50.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██▏       | 90.0M/420M [00:01<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 73.1M/420M [00:01<00:07, 50.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 96.8M/420M [00:01<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  19%|█▊        | 78.0M/420M [00:01<00:07, 50.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 104M/420M [00:01<00:04, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 83.0M/420M [00:01<00:06, 51.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▋       | 110M/420M [00:01<00:04, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 88.2M/420M [00:01<00:06, 52.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 117M/420M [00:01<00:04, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 93.4M/420M [00:01<00:06, 53.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 124M/420M [00:01<00:04, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 98.6M/420M [00:01<00:06, 53.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 131M/420M [00:02<00:04, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 104M/420M [00:02<00:06, 54.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 138M/420M [00:02<00:04, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 109M/420M [00:02<00:05, 54.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 144M/420M [00:02<00:04, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 115M/420M [00:02<00:05, 54.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 151M/420M [00:02<00:03, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  29%|██▊       | 120M/420M [00:02<00:05, 55.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 158M/420M [00:02<00:03, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 125M/420M [00:02<00:05, 55.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 165M/420M [00:02<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 130M/420M [00:02<00:05, 55.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 172M/420M [00:02<00:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 136M/420M [00:02<00:05, 55.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 179M/420M [00:02<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  34%|███▎      | 141M/420M [00:02<00:05, 54.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 185M/420M [00:02<00:03, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 147M/420M [00:02<00:05, 55.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 192M/420M [00:02<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 152M/420M [00:02<00:04, 56.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 199M/420M [00:03<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 158M/420M [00:03<00:04, 57.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 206M/420M [00:03<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 164M/420M [00:03<00:04, 57.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 213M/420M [00:03<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 169M/420M [00:03<00:04, 58.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 219M/420M [00:03<00:02, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 175M/420M [00:03<00:04, 59.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 226M/420M [00:03<00:02, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 181M/420M [00:03<00:04, 59.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 233M/420M [00:03<00:02, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 187M/420M [00:03<00:04, 60.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 240M/420M [00:03<00:02, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 193M/420M [00:03<00:03, 60.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 247M/420M [00:03<00:02, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 198M/420M [00:03<00:03, 60.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 254M/420M [00:03<00:02, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  49%|████▊     | 204M/420M [00:03<00:03, 60.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 260M/420M [00:03<00:02, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 210M/420M [00:03<00:03, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▎   | 267M/420M [00:04<00:02, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  51%|█████▏    | 216M/420M [00:04<00:03, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 274M/420M [00:04<00:02, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 222M/420M [00:04<00:03, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 281M/420M [00:04<00:02, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 227M/420M [00:04<00:03, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▊   | 288M/420M [00:04<00:01, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 233M/420M [00:04<00:03, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 295M/420M [00:04<00:01, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 239M/420M [00:04<00:03, 61.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 302M/420M [00:04<00:01, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 245M/420M [00:04<00:03, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 308M/420M [00:04<00:01, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 251M/420M [00:04<00:02, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 315M/420M [00:04<00:01, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 257M/420M [00:04<00:02, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 322M/420M [00:04<00:01, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 262M/420M [00:04<00:02, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 329M/420M [00:04<00:01, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 268M/420M [00:04<00:02, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 336M/420M [00:05<00:01, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 274M/420M [00:05<00:02, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 343M/420M [00:05<00:01, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 280M/420M [00:05<00:02, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 349M/420M [00:05<00:01, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 286M/420M [00:05<00:02, 61.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 356M/420M [00:05<00:00, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 291M/420M [00:05<00:02, 61.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▋ | 363M/420M [00:05<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 297M/420M [00:05<00:02, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 370M/420M [00:05<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 303M/420M [00:05<00:02, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 377M/420M [00:05<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▎  | 309M/420M [00:05<00:01, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████▏| 384M/420M [00:05<00:00, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 315M/420M [00:05<00:01, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 390M/420M [00:05<00:00, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▋  | 321M/420M [00:05<00:01, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 397M/420M [00:05<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 326M/420M [00:05<00:01, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 404M/420M [00:06<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 332M/420M [00:06<00:01, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 411M/420M [00:06<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 338M/420M [00:06<00:01, 61.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 418M/420M [00:06<00:00, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|██████████| 420M/420M [00:06<00:00, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 344M/420M [00:06<00:01, 61.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 350M/420M [00:06<00:01, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 356M/420M [00:06<00:01, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 361M/420M [00:06<00:01, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 367M/420M [00:06<00:00, 61.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 373M/420M [00:06<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 379M/420M [00:06<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 385M/420M [00:06<00:00, 61.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 391M/420M [00:07<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 397M/420M [00:07<00:00, 61.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 403M/420M [00:07<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 408M/420M [00:07<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▊| 414M/420M [00:07<00:00, 61.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading pytorch_model.bin: 100%|██████████| 420M/420M [00:07<00:00, 58.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO Bootstrap : Using eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:209 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:211 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:344 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:347 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:346 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:213 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.69.54<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:215 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:226 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:228 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:218 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:220 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:227 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:225 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:216 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO Bootstrap : Using eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.94.10<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:693 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 3/-1/-1->0->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 10/-1/-1->11->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] -1/-1/-1->12->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] 14/-1/-1->13->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Trees [0] 11/-1/-1->8->0 [1] 11/0/-1->8->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 13/-1/-1->9->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 00 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 01 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 00 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 01 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 00 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 00 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 01 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 01 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 00 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 00 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO Channel 01 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO Channel 01 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 01 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 01 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO Channel 01 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO Channel 01 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 01 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 01 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 01 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 00 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 01 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 00 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 01 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 01 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO Channel 00 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO Channel 00 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 00 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 00 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO Channel 01 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO Channel 01 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 01 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 01 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 00 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 00 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 01 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO Channel 01 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 01 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO Channel 01 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 01 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 01 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO Channel 01 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO Channel 01 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 00 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO Channel 00 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO Channel 00 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 00 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO Channel 01 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO Channel 01 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:228:859 [6] NCCL INFO comm 0x7f6c74002fb0 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:227:858 [2] NCCL INFO comm 0x7f69b8002fb0 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:226:861 [4] NCCL INFO comm 0x7faaac002fb0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:218:857 [5] NCCL INFO comm 0x7fbe14002fb0 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:225:860 [3] NCCL INFO comm 0x7fdac4002fb0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:216:863 [1] NCCL INFO comm 0x7fc0c4002fb0 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:693:864 [0] NCCL INFO comm 0x7efe80002fb0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:220:862 [7] NCCL INFO comm 0x7f9a20002fb0 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:347:855 [1] NCCL INFO comm 0x7f7888002fb0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:209:852 [5] NCCL INFO comm 0x7f2384002fb0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:344:856 [2] NCCL INFO comm 0x7f6384002fb0 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:346:854 [7] NCCL INFO comm 0x7fc18c002fb0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:211:853 [4] NCCL INFO comm 0x7f29d0002fb0 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:851 [0] NCCL INFO comm 0x7fbebc002fb0 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:215:858 [3] NCCL INFO comm 0x7f37a4002fb0 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:213:857 [6] NCCL INFO comm 0x7f4bb0002fb0 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:679:679 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-08-31 23:12:21.744: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-08-31 23:12:21.806: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-08-31 23:12:21.806: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-08-31 23:12:21.806: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-08-31 23:12:21.806: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-08-31 23:12:21.807: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-08-31 23:12:21.807: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-08-31 23:12:21.807: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-08-31 23:12:21.807: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.21.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.728857\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.728857\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.671632\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.671632\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.680135\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.680135\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.678408\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.678408\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.666730\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.666730\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.687865\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.687865\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.664782\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.664782\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.655371\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.702446\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.675030\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.675030\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.702446\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.655371\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.687981\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.774252\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.774252\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.687981\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.687441\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.687441\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.701578\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.681300\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.701578\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.681300\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Train Epoch: 1 [0/357 (0%)] Loss: 0.694701\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Train Epoch: 1 [0/357 (0%)] Loss: 0.694701\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Average training loss: 0.644485\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Average training loss: 0.627663\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Average training loss: 0.644485\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Average training loss: 0.627663\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Average training loss: 0.640866\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Average training loss: 0.651731\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Average training loss: 0.640866\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Average training loss: 0.640852\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Average training loss: 0.656776\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Average training loss: 0.647437\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Average training loss: 0.647437\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Average training loss: 0.640852\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Average training loss: 0.656776\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Average training loss: 0.651731\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Average training loss: 0.641095\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Average training loss: 0.619728\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Average training loss: 0.636650\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Average training loss: 0.636650\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Average training loss: 0.619728\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Average training loss: 0.641095\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Average training loss: 0.640170\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Average training loss: 0.640170\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Average training loss: 0.653085\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Average training loss: 0.653085\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Average training loss: 0.652641\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Average training loss: 0.652641\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Average training loss: 0.657986\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Average training loss: 0.657986\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Average training loss: 0.640623\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Average training loss: 0.640623\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Average training loss: 0.636839\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Average training loss: 0.636839\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.747788\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Test set: Accuracy: 0.747788\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.750000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Accuracy: 0.750000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Test set: Accuracy: 0.733407\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Test set: Accuracy: 0.751106\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.733407\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.751106\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Test set: Accuracy: 0.761062\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.761062\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.746681\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Test set: Accuracy: 0.746681\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.733407\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Test set: Accuracy: 0.733407\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Test set: Accuracy: 0.742257\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.742257\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Test set: Accuracy: 0.754425\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.754425\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.745575\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Test set: Accuracy: 0.745575\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Test set: Accuracy: 0.754425\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.754425\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.746681\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Test set: Accuracy: 0.746681\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Test set: Accuracy: 0.735619\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.735619\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.758850\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Test set: Accuracy: 0.758850\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.751106\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Test set: Accuracy: 0.751106\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Test set: Accuracy: 0.744469\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.744469\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.627070\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.535135\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.617498\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.601176\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.581305\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.560223\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.581363\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.569680\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.581305\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.601176\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.617498\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.535135\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.627070\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.560223\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.581363\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.569680\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.554097\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.573396\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.635674\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.554097\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.573396\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.583626\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.570894\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.572911\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.575749\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Train Epoch: 2 [0/357 (0%)] Loss: 0.547550\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.635674\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.583626\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.575749\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.572911\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.570894\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Train Epoch: 2 [0/357 (0%)] Loss: 0.547550\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Average training loss: 0.543720\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Average training loss: 0.554452\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Average training loss: 0.543720\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Average training loss: 0.554452\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Average training loss: 0.537493\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Average training loss: 0.537493\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Average training loss: 0.558433\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Average training loss: 0.558433\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Average training loss: 0.565974\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Average training loss: 0.565974\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Average training loss: 0.556266\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Average training loss: 0.556266\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Average training loss: 0.555953\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Average training loss: 0.555953\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Average training loss: 0.537980\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Average training loss: 0.537980\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Average training loss: 0.555897\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Average training loss: 0.564538\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Average training loss: 0.537095\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Average training loss: 0.555897\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Average training loss: 0.564538\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Average training loss: 0.537095\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Average training loss: 0.570166\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Average training loss: 0.570166\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Average training loss: 0.571121\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Average training loss: 0.571121\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Average training loss: 0.535398\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Average training loss: 0.535398\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Average training loss: 0.550327\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Average training loss: 0.550327\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Average training loss: 0.578807\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Average training loss: 0.578807\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Test set: Accuracy: 0.773230\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.773230\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.775442\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Test set: Accuracy: 0.775442\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.797566\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Accuracy: 0.797566\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Test set: Accuracy: 0.772124\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.772124\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.767699\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Test set: Accuracy: 0.767699\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Test set: Accuracy: 0.799779\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.799779\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Test set: Accuracy: 0.799779\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.799779\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Test set: Accuracy: 0.774336\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.774336\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Test set: Accuracy: 0.778761\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.778761\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Test set: Accuracy: 0.778761\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.778761\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Test set: Accuracy: 0.783186\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.783186\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Test set: Accuracy: 0.785398\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Test set: Accuracy: 0.769912\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.769912\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.785398\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.782080\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Test set: Accuracy: 0.782080\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Test set: Accuracy: 0.780973\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Test set: Accuracy: 0.780973\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Test set: Accuracy: 0.787611\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Test set: Accuracy: 0.787611\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Saving tuned model.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:12:35,685 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2022-08-31 23:12:35,658 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:12:35,658 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-31 23:12:35,659 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2022-08-31 23:13:05,715 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2022-08-31 23:13:05,715 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-08-31 23:13:13 Uploading - Uploading generated training model\n",
      "2022-08-31 23:14:59 Completed - Training job completed\n",
      "Training seconds: 1002\n",
      "Billable seconds: 1002\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# 1. Defining the estimator \n",
    "\n",
    "estimator = PyTorch(entry_point=\"train_deploy.py\",\n",
    "                    source_dir=\"code\",\n",
    "                    role=role,\n",
    "                    framework_version=\"1.12.0\",\n",
    "                    py_version=\"py38\",\n",
    "                    instance_count=2,                          # Distributed training for GPU instances.\n",
    "                    instance_type=\"ml.p3.16xlarge\",             # Type of instance we want the training to happen\n",
    "                    hyperparameters={\"epochs\": 2,\n",
    "                                     \"num_labels\": 2,\n",
    "                                     \"backend\": \"nccl\",        # gloo and tcp for cpu instances - gloo and nccl for gpu instances\n",
    "                                    },\n",
    "                    debugger_hook_config=False,  # deactivate debugger to avoid warnings in model artifact\n",
    "                    disable_profiler=True,  # keep running resources to a minimum to avoid permission errors\n",
    "                    distribution={\"pytorchddp\":{\"enabled\": True}},\n",
    "                   )\n",
    "\n",
    "# 2. Start the Training \n",
    "\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
